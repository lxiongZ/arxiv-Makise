# arxiv_assistant.py

import datetime
import email.utils
import os
import smtplib
import time
import xml.etree.ElementTree as ET
from concurrent.futures import ProcessPoolExecutor
from email.header import Header
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

import requests
from jinja2 import Environment, FileSystemLoader
from openai import OpenAI
from tqdm import tqdm


def get_target_date(days_ago=2):
    today = datetime.datetime.now()
    target_date = today - datetime.timedelta(days=days_ago)
    return target_date.strftime("%Y-%m-%d")


def search_arxiv_papers(search_term, target_date, max_results=10):
    papers = []
    base_url = "http://export.arxiv.org/api/query?"
    categories = "(cat:cs.AI+OR+cat:cs.LG+OR+cat:q-bio.*+OR+cat:physics.chem-ph)" # åªè¦å«æœ‰ä¸€ä¸ªç±»åˆ«å³å¯
    search_query = f"search_query=all:{search_term}+AND+{categories}&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending"
    # search_query = f"search_query=all:{search_term}+AND+cat:cs.*&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending"
    url = base_url + search_query

    headers = {"User-Agent": "Mozilla/5.0"}
    for attempt in range(10):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                break
        except requests.exceptions.RequestException as e:
            print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            time.sleep(2)
    else:
        print("è¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä½ çš„æŸ¥è¯¢å‚æ•°æˆ–ç½‘ç»œè¿æ¥ã€‚")
        return []

    root = ET.fromstring(response.content)
    namespaces = {"atom": "http://www.w3.org/2005/Atom", "arxiv": "http://arxiv.org/schemas/atom"}
    entries = root.findall(".//atom:entry", namespaces)
    if not entries:
        return []

    today_str = datetime.datetime.now().strftime("%Y-%m-%d")
    target_date_obj = datetime.datetime.strptime(target_date, "%Y-%m-%d")
    today_obj = datetime.datetime.strptime(today_str, "%Y-%m-%d")

    for entry in entries:
        title = entry.find("./atom:title", namespaces).text.strip()
        summary = entry.find("./atom:summary", namespaces).text.strip()
        url = entry.find("./atom:id", namespaces).text.strip()
        pub_date_str = entry.find("./atom:published", namespaces).text
        pub_date = datetime.datetime.strptime(pub_date_str, "%Y-%m-%dT%H:%M:%SZ").strftime("%Y-%m-%d")
        pub_date_obj = datetime.datetime.strptime(pub_date, "%Y-%m-%d")
        authors = [a.find("./atom:name", namespaces).text.strip() for a in entry.findall("./atom:author", namespaces)]
        arxiv_id = url.split("/")[-1]
        categories = [c.get("term") for c in entry.findall("./atom:category", namespaces)]
        comments_elem = entry.find("./arxiv:comment", namespaces)
        comments = comments_elem.text.strip() if comments_elem is not None and comments_elem.text else None

        if target_date_obj <= pub_date_obj <= today_obj:
            papers.append(
                {
                    "title": title,
                    "authors": authors,
                    "url": url,
                    "arxiv_id": arxiv_id,
                    "pub_date": pub_date,
                    "summary": summary,
                    "categories": categories,
                    "comments": comments,
                }
            )

    print(f"æ‰¾åˆ° {len(papers)} ç¯‡ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ã€‚")

    return papers

# def search_by_combined_categories(target_date, max_results=100):
#     papers = []
#     base_url = "http://export.arxiv.org/api/query?"
#     search_query = f"search_query=cat:cs.AI+OR+cat:cs.LG+OR+cat:q-bio.*&start=0&max_results={max_results}&sortBy=submittedDate&sortOrder=descending"
#     url = base_url + search_query

#     headers = {"User-Agent": "Mozilla/5.0"}
#     try:
#         response = requests.get(url, headers=headers, timeout=10)
#         if response.status_code != 200:
#             print("åˆ†ç±»ç»„åˆæŸ¥è¯¢å¤±è´¥ã€‚")
#             return []
#     except requests.exceptions.RequestException as e:
#         print(f"åˆ†ç±»ç»„åˆè¯·æ±‚å¤±è´¥: {e}")
#         return []

#     root = ET.fromstring(response.content)
#     namespaces = {"atom": "http://www.w3.org/2005/Atom", "arxiv": "http://arxiv.org/schemas/atom"}
#     entries = root.findall(".//atom:entry", namespaces)
#     if not entries:
#         return []

#     today_str = datetime.datetime.now().strftime("%Y-%m-%d")
#     target_date_obj = datetime.datetime.strptime(target_date, "%Y-%m-%d")
#     today_obj = datetime.datetime.strptime(today_str, "%Y-%m-%d")

#     for entry in entries:
#         title = entry.find("./atom:title", namespaces).text.strip()
#         summary = entry.find("./atom:summary", namespaces).text.strip()
#         url = entry.find("./atom:id", namespaces).text.strip()
#         pub_date_str = entry.find("./atom:published", namespaces).text
#         pub_date = datetime.datetime.strptime(pub_date_str, "%Y-%m-%dT%H:%M:%SZ").strftime("%Y-%m-%d")
#         pub_date_obj = datetime.datetime.strptime(pub_date, "%Y-%m-%d")
#         authors = [a.find("./atom:name", namespaces).text.strip() for a in entry.findall("./atom:author", namespaces)]
#         arxiv_id = url.split("/")[-1]
#         categories = [c.get("term") for c in entry.findall("./atom:category", namespaces)]
#         comments_elem = entry.find("./arxiv:comment", namespaces)
#         comments = comments_elem.text.strip() if comments_elem is not None and comments_elem.text else None

#         if target_date_obj <= pub_date_obj <= today_obj:
#             # æ£€æŸ¥æ˜¯å¦åŒæ—¶åŒ…å« AI/LG å’Œ q-bio
#             has_ai_or_lg = any(cat in ["cs.AI", "cs.LG"] for cat in categories)
#             has_qbio = any(cat.startswith("q-bio.") for cat in categories)
            
#             if has_ai_or_lg and has_qbio:
#                 papers.append({
#                     "title": title,
#                     "authors": authors,
#                     "url": url,
#                     "arxiv_id": arxiv_id,
#                     "pub_date": pub_date,
#                     "summary": summary,
#                     "categories": categories,
#                     "comments": comments,
#                 })

#     print(f"ç»„åˆåˆ†ç±»æ£€ç´¢æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ã€‚")
#     return papers


def process_with_openai(text, prompt_template, openai_api_key, model_name="gpt-3.5-turbo", api_base=None):
    prompt = prompt_template.format(text=text)
    try:
        client_kwargs = {"api_key": openai_api_key}
        if api_base:
            client_kwargs["base_url"] = api_base
        client = OpenAI(**client_kwargs)
        response = client.chat.completions.create(
            model=model_name, messages=[{"role": "user", "content": prompt}], temperature=1.3, max_tokens=8192
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"å¤„ç†æ–‡æœ¬æ—¶å‡ºç°é”™è¯¯: {e}")
        return f"å¤„ç†å¤±è´¥: {str(e)}"


def process_paper_with_openai(paper, translation_prompt, contribution_prompt, api_key, model, api_base):
    translated_summary = process_with_openai(paper["summary"], translation_prompt, api_key, model, api_base)
    contribution_summary = process_with_openai(paper["summary"], contribution_prompt, api_key, model, api_base)
    return (paper["arxiv_id"], translated_summary, contribution_summary)


def render_email_html(papers_by_keyword, keywords, today, date_range):
    env = Environment(loader=FileSystemLoader("templates"))
    template = env.get_template("email.html")
    return template.render(
        today=today,
        keywords=", ".join(keywords),
        date_range=date_range,
        keyword_papers=papers_by_keyword,
        total_papers=sum(len(p) for p in papers_by_keyword.values()),
    )


def send_email(
    subject,
    html_content,
    sender_email,
    sender_password,
    receiver_emails,
    sender_name=None,
    smtp_server="smtp.qq.com",
    smtp_port=465,
):
    message = MIMEMultipart("alternative")
    if sender_name:
        message["From"] = email.utils.formataddr((sender_name, sender_email))
    else:
        message["From"] = sender_email

    if isinstance(receiver_emails, list):
        message["To"] = ", ".join(receiver_emails)
        recipients = receiver_emails
    else:
        message["To"] = receiver_emails
        recipients = [receiver_emails]

    message["Subject"] = Header(subject, "utf-8")
    message.attach(MIMEText("è¯·ä½¿ç”¨æ”¯æŒHTMLçš„é‚®ç®±æŸ¥çœ‹æ­¤å†…å®¹ã€‚", "plain", "utf-8"))
    message.attach(MIMEText(html_content, "html", "utf-8"))

    try:
        server = smtplib.SMTP_SSL(smtp_server, smtp_port)
        server.login(sender_email, sender_password)
        server.sendmail(sender_email, recipients, message.as_string())
        server.quit()
        print(f"é‚®ä»¶å·²æˆåŠŸå‘é€è‡³ {', '.join(recipients)}")
    except Exception as e:
        print(f"å‘é€é‚®ä»¶æ—¶å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    SENDER_EMAIL = os.environ.get("SENDER_EMAIL")
    SENDER_NAME = os.environ.get("SENDER_NAME", "arXivè®ºæ–‡åŠ©æ‰‹")
    SENDER_PASSWORD = os.environ.get("SENDER_PASSWORD")
    RECEIVER_EMAILS = os.environ.get("RECEIVER_EMAILS", "").split(",")
    SMTP_SERVER = os.environ.get("SMTP_SERVER", "smtp.qq.com")
    SMTP_PORT = int(os.environ.get("SMTP_PORT", "465"))

    OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
    OPENAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-3.5-turbo")
    OPENAI_API_BASE = os.environ.get("OPENAI_API_BASE")

    TRANSLATION_PROMPT = """æˆ‘å°†ç»™ä½ ä¸€ä¸ªäººå·¥æ™ºèƒ½é¢†åŸŸçš„è®ºæ–‡æ‘˜è¦ï¼Œä½ éœ€è¦ç¿»è¯‘æˆä¸­æ–‡ï¼Œæ³¨æ„é€šé¡ºæµç•…ï¼Œé¢†åŸŸä¸“æœ‰ç”¨è¯­ï¼ˆå¦‚transformer, token, logitï¼‰ä¸ç”¨ç¿»è¯‘ã€‚è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸éœ€è¦Markdownæ ¼å¼ã€‚\n{text}"""
    CONTRIBUTION_PROMPT = """æˆ‘å°†ç»™ä½ ä¸€ä¸ªäººå·¥æ™ºèƒ½é¢†åŸŸçš„è®ºæ–‡æ‘˜è¦ï¼Œä½ éœ€è¦ä½¿ç”¨ä¸­æ–‡ï¼Œå°†æœ€æ ¸å¿ƒçš„å†…å®¹ç”¨ä¸€å¥è¯è¯´æ˜ï¼Œä¸€èˆ¬æ ¼å¼ä¸ºï¼šç”¨äº†ä»€ä¹ˆåŠæ³•è§£å†³äº†ä»€ä¹ˆé—®é¢˜ã€‚æ³¨æ„é€šé¡ºæµç•…ï¼Œé¢†åŸŸä¸“æœ‰ç”¨è¯­ï¼ˆå¦‚transformer, token, logitï¼‰ä¸ç”¨ç¿»è¯‘ã€‚è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸éœ€è¦Markdownæ ¼å¼ã€‚\n{text}"""

    search_terms_str = os.environ.get("SEARCH_TERMS", '"transformer","large language model"')
    search_terms = [term.strip() for term in search_terms_str.split(",")]
    max_results = int(os.environ.get("MAX_RESULTS", "10"))
    days_ago = int(os.environ.get("DAYS_AGO", "30")) # ä¿®æ”¹ä¸º7å¤©çš„

    target_date = get_target_date(days_ago)
    today_date = datetime.datetime.now().strftime("%Y-%m-%d")

    all_papers = {}
    keyword_papers = {}

    for search_term in tqdm(search_terms):
        papers = search_arxiv_papers(search_term, target_date, max_results)
        keyword_papers[search_term] = []
        for paper in papers:
            if paper["arxiv_id"] not in all_papers:
                all_papers[paper["arxiv_id"]] = paper
            keyword_papers[search_term].append(paper["arxiv_id"])
    
    # # ğŸ” æ·»åŠ ç»„åˆåˆ†ç±»æ£€ç´¢ï¼ˆä¸ä½¿ç”¨å…³é”®è¯ï¼‰
    # combined_papers = search_by_combined_categories(target_date, max_results * 2)
    # keyword_papers["åˆ†ç±»ç»„åˆï¼ˆAI+q-bio æˆ– LG+q-bioï¼‰"] = []

    # for paper in combined_papers:
    #     if paper["arxiv_id"] not in all_papers:
    #         all_papers[paper["arxiv_id"]] = paper
    #     keyword_papers["åˆ†ç±»ç»„åˆï¼ˆAI+q-bio æˆ– LG+q-bioï¼‰"].append(paper["arxiv_id"])

    if not all_papers:
        print("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ã€‚")
        exit()

    with ProcessPoolExecutor(max_workers=16) as executor:
        futures = [
            executor.submit(
                process_paper_with_openai,
                paper,
                TRANSLATION_PROMPT,
                CONTRIBUTION_PROMPT,
                OPENAI_API_KEY,
                OPENAI_MODEL,
                OPENAI_API_BASE,
            )
            for paper in all_papers.values()
        ]
        for future in futures:
            arxiv_id, translated, contribution = future.result()
            all_papers[arxiv_id]["translated_summary"] = translated
            all_papers[arxiv_id]["contribution"] = contribution

    papers_by_keyword = {}
    for keyword, arxiv_ids in keyword_papers.items():
        papers_by_keyword[keyword] = [
            {
                **all_papers[pid],
                "authors": ", ".join(all_papers[pid]["authors"]),
                "categories": ", ".join(all_papers[pid]["categories"]),
            }
            for pid in arxiv_ids
        ]

    html_content = render_email_html(papers_by_keyword, search_terms, today_date, f"{target_date} - {today_date}")

    send_email(
        f"arXivè®ºæ–‡æ—¥æŠ¥ - {today_date} - {len(all_papers)}ç¯‡è®ºæ–‡",
        html_content,
        SENDER_EMAIL,
        SENDER_PASSWORD,
        RECEIVER_EMAILS,
        SENDER_NAME,
        SMTP_SERVER,
        SMTP_PORT,
    )
